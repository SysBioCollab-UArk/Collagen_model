"""
Generated by pydream_it
PyDREAM run script for Col_model.py 
"""
from pydream.core import run_dream
from pysb.simulator import ScipyOdeSimulator
import numpy as np
from pydream.parameters import SampledParam
from pydream.convergence import Gelman_Rubin
from scipy.stats import norm, uniform
from Col_model import model
import os
import re

# DREAM Settings
# Number of chains - should be at least 3.
nchains = 5
# Number of iterations
niterations = 5000

# Initialize PySB solver object for running simulations. Simulation timespan should match experimental data.
files = sorted(os.listdir('.'))
exp_time_files = [f for f in files if re.search(r'exp_data_time_\d+', f)]
experiments_time = [np.genfromtxt(file, delimiter=',', names=True) for file in exp_time_files]
n_experiments = len(experiments_time)
tspan = []
tspan_mask = []
for exp_time in experiments_time:
    tspan.append([])
    for name in exp_time.dtype.names:
        tspan[-1] += [t for t in exp_time[name] if not np.isnan(t)]
    tspan[-1] = sorted(list(set(tspan[-1])))  # get a common set of time points for simulations
    tspan_mask.append({})  # for each species, need to mark which time points we have data for
    for name in exp_time.dtype.names:
        tspan_mask[-1][name] = [False] * len(tspan[-1])
        for i in range(len(tspan[-1])):
            if tspan[-1][i] in exp_time[name]:
                tspan_mask[-1][name][i] = True
solver = ScipyOdeSimulator(model)
parameters_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
rates_mask = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]
param_values = np.array([p.value for p in model.parameters])

# USER must add commands to import/load any experimental data for use in the likelihood function!
exp_avg_files = [f for f in files if re.search(r'exp_data_avg_\d+', f)]
experiments_avg = [np.genfromtxt(file, delimiter=',', names=True) for file in exp_avg_files]
exp_se_files = [f for f in files if re.search(r'exp_data_se_\d+', f)]
experiments_se = [np.genfromtxt(file, delimiter=',', names=True) for file in exp_se_files]
like_data = []
for exp_avg, exp_se in zip(experiments_avg, experiments_se):
    like_data.append({})
    for name in exp_avg.dtype.names:
        # remove any nans, which will happen if the time points are different for different species
        avg = [e for e in exp_avg[name] if not np.isnan(e)]
        se = [e for e in exp_se[name] if not np.isnan(e)]
        like_data[-1][name] = norm(loc=avg, scale=se)


# USER must define a likelihood function!
def likelihood(position):
    y = np.copy(position)
    logp_data = [0] * n_experiments
    for n in range(n_experiments):
        param_values[rates_mask] = 10 ** y
        sim = solver.run(tspan=tspan[n], param_values=param_values).all
        for sp in like_data[n].keys():
            logp_data[n] += np.sum(like_data[n][sp].logpdf(sim[sp][tspan_mask[n][sp]]))
        if np.isnan(logp_data[n]):
            logp_data[n] = -np.inf
    return sum(logp_data)


sampled_params_list = list()
sampled_params_list.append(SampledParam(norm, loc=np.log10(2.35), scale=2.0))  # M1_init
sampled_params_list.append(SampledParam(norm, loc=np.log10(363.12), scale=2.0))  # M2_init
sampled_params_list.append(SampledParam(norm, loc=np.log10(2.08), scale=2.0))  # M8_init
sampled_params_list.append(SampledParam(norm, loc=np.log10(85.57), scale=2.0))  # M9_init
sampled_params_list.append(SampledParam(norm, loc=np.log10(75.16), scale=2.0))  # T1_init
sampled_params_list.append(SampledParam(norm, loc=np.log10(81.74), scale=2.0))  # T2_init
sampled_params_list.append(SampledParam(norm, loc=np.log10(0.97), scale=2.0))  # T4_init
sampled_params_list.append(SampledParam(norm, loc=np.log10(44.58), scale=2.0))  # C1_init
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1C1d
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2C1d
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8C1d
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9C1d
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1i
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2i
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8i
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9i
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # T1i
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # T2i
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # T4i
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1C1f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2C1f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8C1f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9C1f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1T1f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2T1f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8T1f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9T1f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1T2f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2T2f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8T2f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9T2f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1T4f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2T4f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8T4f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9T4f
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1C1r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2C1r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8C1r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9C1r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1T1r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2T1r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8T1r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9T1r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1T2r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2T2r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8T2r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9T2r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M1T4r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M2T4r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M8T4r
sampled_params_list.append(SampledParam(norm, loc=np.log10(1.0), scale=2.0))  # M9T4r

if __name__ == '__main__':

    sampled_params, log_ps = run_dream(parameters=sampled_params_list,
                                       likelihood=likelihood,
                                       niterations=niterations,
                                       nchains=nchains,
                                       multitry=False,
                                       gamma_levels=4,
                                       adapt_gamma=True,
                                       history_thin=1,
                                       model_name='dreamzs_%dchain' % nchains,
                                       verbose=True)
    total_iterations = niterations
    burnin = int(total_iterations / 2)
    # Save sampling output (sampled parameter values and their corresponding logps).
    for chain in range(len(sampled_params)):
        np.save('dreamzs_%dchain_sampled_params_chain_%d_%d' %
                (nchains, chain, total_iterations), sampled_params[chain])
        np.save('dreamzs_%dchain_logps_chain_%d_%d' % (nchains, chain, total_iterations), log_ps[chain])
    old_samples = sampled_params

    # Check convergence and continue sampling if not converged
    GR = Gelman_Rubin(sampled_params)
    print('At iteration: ', total_iterations, ' GR = ', GR)
    np.savetxt('dreamzs_%dchain_GelmanRubin_iteration_%d.txt' % (nchains, total_iterations), GR)
    if np.any(GR > 1.2):
        starts = [sampled_params[chain][-1, :] for chain in range(nchains)]
        converged = False
        while not converged:
            total_iterations += niterations
            burnin += niterations
            sampled_params, log_ps = run_dream(parameters=sampled_params_list,
                                               likelihood=likelihood,
                                               niterations=niterations,
                                               nchains=nchains,
                                               start=starts,
                                               multitry=False,
                                               gamma_levels=4,
                                               adapt_gamma=True,
                                               history_thin=1,
                                               model_name='dreamzs_%dchain' % nchains,
                                               verbose=True,
                                               restart=True)
            for chain in range(len(sampled_params)):
                np.save('dreamzs_%dchain_sampled_params_chain_%d_%d' %
                        (nchains, chain, total_iterations), sampled_params[chain])
                np.save('dreamzs_%dchain_logps_chain_%d_%d' % (nchains, chain, total_iterations), log_ps[chain])
            old_samples = [np.concatenate((old_samples[chain], sampled_params[chain])) for chain in range(nchains)]
            GR = Gelman_Rubin(old_samples)
            print('At iteration: ', total_iterations, ' GR = ', GR)
            np.savetxt('dreamzs_%dchain_GelmanRubin_iteration_%d.txt' % (nchains, total_iterations), GR)
            if np.all(GR < 1.2):
                converged = True

    # Plot output
    try:
        from pydream_it import plot_param_dist, plot_log_likelihood, plot_time_courses, \
            get_unique_samples_for_simulation

        total_iterations = len(old_samples[0])
        # parameter distributions
        print('Plotting parameter distributions')
        samples = np.concatenate(tuple([old_samples[chain][burnin:, :] for chain in range(nchains)]))
        for n in range(n_experiments):
            samples_n = samples  # TODO: Add code to get samples for the nth experiment
            plot_param_dist(samples_n, [model.parameters[i].name for i in parameters_idxs],
                            suffix='_exp_%d' % n)
        # log likelihood
        print('Plotting log-likelihoods')
        log_ps = []
        n_files = int(total_iterations / niterations)
        for chain in range(nchains):
            log_ps.append(np.concatenate(
                tuple(np.load('dreamzs_%dchain_logps_chain_%d_%d.npy' % (nchains, chain, niterations * (i+1))).flatten()
                      for i in range(n_files))))
        plot_log_likelihood(log_ps, cutoff=2)
        # time courses
        print('Plotting time courses')
        log_ps = np.concatenate(tuple(log_ps[i][burnin:] for i in range(nchains)))
        for n in range(n_experiments):
            print('Experiment %d' % n)
            tspan = np.linspace(tspan[n][0], tspan[n][-1], int((tspan[n][-1] - tspan[n][0]) * 10 + 1))
            samples_n = samples  # TODO: Add code to get samples for the nth experiment
            samples_n, counts = get_unique_samples_for_simulation(samples_n, log_ps, cutoff=2)
            param_values = np.array([param_values] * len(samples_n))
            for i in range(len(param_values)):
                param_values[i][parameters_idxs] = 10 ** samples_n[i]
            print('Running %d simulations' % len(param_values))
            output_all = solver.run(tspan=tspan, param_values=param_values).all
            plot_time_courses(experiments_avg[n].dtype.names, tspan, output_all, counts=counts,
                              exp_data=(experiments_time[n], experiments_avg[n], experiments_se[n]),
                              suffix='_exp_%d' % n)
        print('DONE')

    except ImportError:
        pass

else:
    run_kwargs = {'parameters': sampled_params_list, 'likelihood': likelihood, 'niterations': niterations,
                  'nchains': nchains, 'multitry': False, 'gamma_levels': 4, 'adapt_gamma': True, 'history_thin': 1,
                  'model_name': 'dreamzs_%dchain' % nchains, 'verbose': True}
